# Before

Shell 0:
* Download HotSpot and GraalVM/Mandrel JDKs.
* Build JMH branch, make sure it's built with HotSpot and not GraalVM, otherwise no benchmark code generated.
* Build Fibula.
* Build JDK 21 with disassembler.

Shell 1:
```shell
benchmarking-linux
maven-21
cd jmh
mvn clean package -DskipTests
```

Shell 2:
```shell
benchmarking-linux
graal-21
cd fibula
mvn clean package -DskipTests -Pnative -Dquarkus.package.jar.decompiler.enabled=true -Dquarkus.native.debug.enabled -Dfibula.native.additional-build-args=-H:-DeleteLocalSymbols
```

Optional shell 3:
```shell
export JAVA_HOME=$HOME/1/jdk21u-dev/build/release-linux-x86_64/jdk
maven-java
cd fibula-jvm
mvn clean package -DskipTests
```

# Record

Press record!

# First JMH benchmark

Show JMH benchmark in IDE.

From the `jmh` folder:
```shell
java -jar target/benchmarks.jar MyFirst -f 1 -i 1 -wi 1 -r 1 -w 1 -tu us
```

Note to audience: VM version, VM invoker

# First Fibula benchmark

Demonstrate that the benchmark is exactly the same as before in the IDE.

From the `fibula` folder:
```shell
java -jar target/benchmarks.jar MyFirst -f 1 -i 1 -wi 1 -r 1 -w 1 -tu us
```

# Build Time

What exactly are these benchmark executing?

If we look at JMH, we see that it generates source code,
compiles it,
and then wraps it around code to capture metrics.
E.g. `./jmh/target/generated-sources/annotations/org/sample/jmh_generated/MyFirstBenchmark_helloWorld_jmhTest.java`

Fibula uses the exact same source code that JMH generates it,
e.g. `./fibula/target/generated-sources/bc/org/sample/jmh_generated/MyFirstBenchmark_helloWorld_jmhTest.java`,
but the way this source is generated is slightly different to a typical JMH application.

Normally JMH applications use annotation processing integration to achieve this.
Fibula could have used the same annotation processing,
but it would have required bridging over to Jandex,
which is where annotations metadata is stored in Quarkus applications.

Instead, Fibula uses JMH's reflection bytecode generator to generate all the source code.
These generator has an API which takes in as main parameter the directory where the benchmark code is compiled,
and upon invocation the source code is generated.

Then, Fibula's Quarkus integrations compiles that java source code into bytecode using the in-memory Java compiler,
and that gets fed into the generated bytecode within Quarkus.

By doing this, the generated benchmark bytecode becomes part of the application,
which then can either run as JVM or native application.

For more details build time details, see
[Fibula's README file](https://github.com/galderz/fibula/blob/main/README.md).

# Run Time

At run time Fibula leverages all of existing JMH capabilities.

Launching `benchmarks.jar` launches JMH's `org.openjdk.jmh.runner.Runner`
with some small tweaks to be able to present the right information,
and be able to launch the native executable.

The native executable is a thin wrapper that just calls `org.openjdk.jmh.runner.ForkedMain`.

The `Runner` and `ForkedMain` processes communicate with each other using JMH's binary client/server architecture.

# Why The Difference In Performance?

Use profiling to understand the differences in performance.

Start with the `perf` profiler
, which runs the forked execution via `perf stat`.

From the `jmh` folder:
```shell
java -jar target/benchmarks.jar MyFirst -f 1 -i 1 -wi 1 -r 1 -w 1 -tu us -prof perf
```

The `perf stat` output values might not very precise due to multiplexing.
Multiplexing happens when multiple counters are tracked with a single hardware counter.
To avoid this, let's limit track `perf` stats to `branches` and `instructions`:

From the `jmh` folder:
```shell
java -jar target/benchmarks.jar MyFirst -f 1 -i 1 -wi 1 -r 1 -w 1 -tu ns -prof perf:events=branches,instructions,cycles
```

We have got some branch and instructions numbers.
Let's try to compare them with Fibula.

From the `fibula` folder:
```shell
java -jar target/benchmarks.jar MyFirst -f 1 -i 1 -wi 1 -r 1 -w 1 -tu ns -prof perf:events=branches,instructions,cycles
```

JMH shows more branches and more instructions,
but the number of operations executed is higher.

It's hard to see the issue from just looking at this data.

To find more meaningful data,
normalize the counters to the number of operations executed.

JMH has the `perfnorm` profiler that does exactly that.

From the `jmh` folder:
```shell
java -jar target/benchmarks.jar MyFirst -f 1 -i 1 -wi 1 -r 1 -w 1 -tu ns -prof perfnorm:events=branches,instructions,cycles
```

Output:
```shell
MyFirstBenchmark.helloWorld:branches      thrpt         1.022               #/op
MyFirstBenchmark.helloWorld:cycles        thrpt         2.051               #/op
MyFirstBenchmark.helloWorld:instructions  thrpt         6.132               #/op
```

From the `fibula` folder:
```shell
java -jar target/benchmarks.jar MyFirst -f 1 -i 1 -wi 1 -r 1 -w 1 -tu ns -prof perfnorm:events=branches,instructions,cycles
```

Output:
```shell
MyFirstBenchmark.helloWorld:branches      thrpt         2.007               #/op
MyFirstBenchmark.helloWorld:cycles        thrpt         6.040               #/op
MyFirstBenchmark.helloWorld:instructions  thrpt         5.029               #/op
```

More branching and more cycles is the reason why SubstrateVM performs worse than HotSpot,
but what are these additional branches? And why more cycles?

`perfasm` is an additional JMH profiler than help uncover this mystery.

Tweak `perfasm` parameters to track `cycles:P` instead of default `cycles`.
The additional `:P` increases the precision of `perf record` by avoiding skidding problems.

From the `jmh` folder:
```shell
java -jar target/benchmarks.jar MyFirst -f 1 -i 1 -wi 1 -r 1 -w 1 -tu us -prof perfasm:events=cycles:P
```

1 branch and 6 instructions clearly visible in the output.

The branch is just the check of `isDone` to see if we have gone past the benchmark running time:
```bash
   2.52%  ↗  0x00007fe5dcb30251:   movzbl		0x94(%r11), %r10d   ;*getfield isDone {reexecute=0 rethrow=0 return_oop=0}
          │                                                            ; - org.sample.jmh_generated.MyFirstBenchmark_helloWorld_jmhTest::helloWorld_thrpt_jmhStub@25 (line 123)
  10.27%  │  0x00007fe5dcb30259:   movq		0x450(%r15), %r8
  15.70%  │  0x00007fe5dcb30260:   addq		$1, %r13            ; ImmutableOopMap {r11=Oop rbx=Oop r14=Oop }
          │                                                            ;*ifeq {reexecute=1 rethrow=0 return_oop=0}
          │                                                            ; - (reexecute) org.sample.jmh_generated.MyFirstBenchmark_helloWorld_jmhTest::helloWorld_thrpt_jmhStub@28 (line 123)
  26.55%  │  0x00007fe5dcb30264:   testl		%eax, (%r8)         ;   {poll}
          │  0x00007fe5dcb30267:   testl		%r10d, %r10d
   0.19%  ╰  0x00007fe5dcb3026a:   je		0x7fe5dcb30251      ;*ifeq {reexecute=0 rethrow=0 return_oop=0}
                                                                       ; - org.sample.jmh_generated.MyFirstBenchmark_helloWorld_jmhTest::helloWorld_thrpt_jmhStub@28 (line 123)
```

Something similar can be achieved with Fibula,
but instead of using the default `perfasm`,
we use a custom profiler that extends `perfasm`
called `org.mendrugo.fibula.bootstrap.DwarfPerfAsmProfiler`.
This profiler makes the following changes:

* Tweaks the `perf record` invocation in `perfasm` to add DWARF callgraph.
* Skips the assembly mapping part because there's no integration for that yet with Fibula.
* Saves the perf binary file separately so that it can be post-processed with `perf annotate`. 

From the `fibula` folder:
```bash
java -jar target/benchmarks.jar MyFirst -f 1 -i 1 -wi 1 -r 1 -w 1 -tu us -prof org.mendrugo.fibula.bootstrap.DwarfPerfAsmProfiler:events=cycles:P
```

Now run `perf annotate`:
```shell
perf annotate -i org.sample.MyFirstBenchmark.helloWorld-Throughput.perfbin
```

The `cmpb+jne` and the `jmp` at the end is the branch for the `isDone` check:

```shell
       │    ┌──cmpb       $0x0,0x58(%rdi)
 18.81 │    ├──jne        dc
 81.19 │    │  subl       $0x1,0x10(%r15)
       │    │↑ jg         c0
       │    │→ callq      _ZN36com.oracle.svm.core.thread.Safepoint27enterSlowPathSafepointCheckEJvv
       │    │  nop
       │    │↑ jmp        c0
       │ dc:└─→mov        %rax,0x20(%rsp)
```

```shell
       │ c0:┌─→inc        %rax
       │    │  cmpb       $0x0,0x58(%rdi)
 18.81 │    │↓ jne        dc
 81.19 │    │  subl       $0x1,0x10(%r15)
       │    │↑ jg         c0
       │    │→ callq      _ZN36com.oracle.svm.core.thread.Safepoint27enterSlowPathSafepointCheckEJvv
       │    │  nop
       │    └──jmp        c0
```

The additional 3rd branch is the `subl+jle` for the safepoint checks:

```shell
       │ c0:┌─→inc        %rax
       │    │  cmpb       $0x0,0x58(%rdi)
 18.81 │    │↓ jne        dc
 81.19 │    ├──subl       $0x1,0x10(%r15)
       │    └──jg         c0
       │     → callq      _ZN36com.oracle.svm.core.thread.Safepoint27enterSlowPathSafepointCheckEJvv
```

This is a very contrived example,
but it gives an idea on what the type of assembly SubstrateVM generates compared to HotSpot.

In other examples I've run I've not seen differences between SubstrateVM and HotSpot,
so the noise we see here might not be so relevant.

In any case, some interesting observations can be made:

* SubstrateVM does dead-code-eliminate the empty method.
* But the safepoint check is not, should it also be dead-code-eliminated?
* Safepoint checks are not as fancy in SubstrateVM as in HotSpot,
  where instead of littering the code with branches,
  it [uses good/bad pages to avoid the branches](https://foojay.io/today/the-inner-workings-of-safepoints/).
* To understand the additional cycles,
  we need to look at how the CPU executes this instructions:
  * [SubstrateVM uops.info](https://uica.uops.info/?code=loop%3A%0D%0Acmpb%20%20%20%20%20%20%240x0%2C0xc(%25rsi)%0D%0Ajne%20%20%20%20%20%20%20b0%0D%0Amov%20%20%20%20%20%20%20%25rax%2C%25rcx%0D%0Ainc%20%20%20%20%20%20%20%25rcx%0D%0Asubl%20%20%20%20%20%20%240x1%2C0x10(%25r15)%0D%0Ajle%20%20%20%20%20%20%20d9%0D%0Amov%20%20%20%20%20%20%20%25rcx%2C%25rax%0D%0Ajmp%20%20%20%20%20%20%20loop%0D%0A&syntax=asATT&uArchs=SNB&tools=uiCA&alignment=0&uiCAHtmlOptions=traceTable&uiCAHtmlOptions=dependencies)
  * [HotSpot uops.info](https://uica.uops.info/?code=loop%3A%0D%0Amovzbl%09%090x94(%25r13)%2C%20%25r10d%0D%0Amovq%09%090x450(%25r15)%2C%20%25r8%0D%0Aaddq%09%09%241%2C%20%25r11%0D%0Atestl%09%09%25eax%2C%20(%25r8)%0D%0Atestl%09%09%25r10d%2C%20%25r10d%0D%0Aje%09%09loop%0D%0A&syntax=asATT&uArchs=SNB&tools=uiCA&alignment=0&uiCAHtmlOptions=traceTable&uiCAHtmlOptions=dependencies)

# Fibula Outings

Some real life examples using Fibula:

* **Feb 2024**: Records equals/hashCode performance.
Initially performance was very bad when records support came out in SubstrateVM.
Christian improved the performance with this [PR](https://github.com/oracle/graal/pull/8109).
A JMH benchmark running with Fibula was able to confirm that the performance had improved significantly:

```shell
Benchmark                                  Mode  Cnt          Score         Error  Units
equalsPositions   GraalVM 24.0.0          thrpt    4  114064530.931 ±  303317.241  ops/s
hashcodePosition  GraalVM 24.0.0          thrpt    4  262827839.960 ± 4187526.946  ops/s
equalsPositions   GraalVM 23.1.0          thrpt    4     117738.738 ±    1891.331  ops/s
hashcodePosition  GraalVM 23.1.0          thrpt    4     292367.045 ±    6803.486  ops/s
```

* **Apr 20204**: [Franz wanted to know if calling `Thread.isVirtual` via method handle instead of direct call would cause a regression in Substrate](https://github.com/quarkusio/quarkus/pull/39704/files#r1547368644).
Fibula showed that both approaches were as fast as each other,
assuming a constant method handle definition:

```shell
FibulaSample_07_IsVirtualMH.directCall        thrpt    4  1176840295.265 ± 46032071.001  ops/s
FibulaSample_07_IsVirtualMH.methodHandleCall  thrpt    4  1166157720.139 ± 59339014.156  ops/s
```

* **June 2024**: A quick experiment building with `-H:+SourceLevelDebug` shows that it has a performance impact:

```shell
MyFirstBenchmark.helloWorld  -H:-SourceLevelDebug thrpt       1845624344.628          ops/s
MyFirstBenchmark.helloWorld  -H:+SourceLevelDebug thrpt       1640804740.323          ops/s
```

# Summary

Fibula allows you to run JMH benchmarks as GraalVM native executables.

# Fibula JVM mode

Fibula can also run in JVM mode.
This can be useful to detect any issues with Fibula itself:

```shell
cd fibula-jvm
java -jar target/benchmarks.jar MyFirst -f 1 -i 1 -wi 1 -r 1 -w 1
```

The results above are equal to JMH,
so the differences in performance cannot be atributted to generated bytecode shape.
